\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{talmor2019commonsenseqa}
\citation{devlin2019bert}
\citation{radford2019language}
\citation{lan2020albert}
\citation{zhang-chai-2010-towards}
\citation{hendrycks2020gaussian}
\citation{liaw2018tune}
\citation{zhang-chai-2009-know}
\citation{vaswani2017attention}
\citation{liu2019roberta}
\citation{N18-1101}
\citation{agarap2019deep}
\citation{ott2019fairseq}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{font-table}{{1}{5}{We compare our base model performance to the default roBERTa model on the EAT task and a baseline which always predicts the same class. Only the classification head is optimized and these base models are kept frozen during training.\relax }{table.caption.4}{}}
\citation{rajani2019explain}
\bibdata{acl2020}
\bibcite{agarap2019deep}{{1}{2019}{{Agarap}}{{}}}
\bibcite{devlin2019bert}{{2}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{hendrycks2020gaussian}{{3}{2020}{{Hendrycks and Gimpel}}{{}}}
\bibcite{lan2020albert}{{4}{2020}{{Lan et~al.}}{{Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}}}
\bibcite{liaw2018tune}{{5}{2018}{{Liaw et~al.}}{{Liaw, Liang, Nishihara, Moritz, Gonzalez, and Stoica}}}
\bibcite{liu2019roberta}{{6}{2019}{{Liu et~al.}}{{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}}}
\bibcite{ott2019fairseq}{{7}{2019}{{Ott et~al.}}{{Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and Auli}}}
\bibcite{radford2019language}{{8}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{rajani2019explain}{{9}{2019}{{Rajani et~al.}}{{Rajani, McCann, Xiong, and Socher}}}
\bibcite{talmor2019commonsenseqa}{{10}{2019}{{Talmor et~al.}}{{Talmor, Herzig, Lourie, and Berant}}}
\bibcite{vaswani2017attention}{{11}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{N18-1101}{{12}{2018}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{zhang-chai-2009-know}{{13}{2009}{{Zhang and Chai}}{{}}}
\bibcite{zhang-chai-2010-towards}{{14}{2010}{{Zhang and Chai}}{{}}}
\bibstyle{acl_natbib}
\gdef \@abspage@last{7}
