{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAT",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad05459152aa46e38a66ee3c3376c21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6d30ee506e545908a15e06663812552",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba1cca0d307f47c7943de351697be09d",
              "IPY_MODEL_db4efe0c16ef49c59ecb7d54e4936220"
            ]
          }
        },
        "b6d30ee506e545908a15e06663812552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba1cca0d307f47c7943de351697be09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f270904d9f4a432d8f51427b89c98b58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec415dab86e44d8aa082406cdca2d311"
          }
        },
        "db4efe0c16ef49c59ecb7d54e4936220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_327e487f673d4560ab7b323cd3a05c03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 940/? [05:10&lt;00:00,  3.02it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_359ac9f8823945688b0044264836ee68"
          }
        },
        "f270904d9f4a432d8f51427b89c98b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec415dab86e44d8aa082406cdca2d311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "327e487f673d4560ab7b323cd3a05c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "359ac9f8823945688b0044264836ee68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b12b60ed0e14d7894f08bd62c78341f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c23502029a0b4978818821d777c01bc1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60bc6ebe503e445c877aed14d3989158",
              "IPY_MODEL_237f12db3ba5460f83d5b9f3edb93082"
            ]
          }
        },
        "c23502029a0b4978818821d777c01bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60bc6ebe503e445c877aed14d3989158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a77b66da9224fd289ec33b445d2cf3e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5af422ad7f6d4e9385ca963b0fc81fa5"
          }
        },
        "237f12db3ba5460f83d5b9f3edb93082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_324ea3ed7ea34d52a6f3a94ba50bfaa0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/? [00:06&lt;00:00,  3.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d70771e0cb0942b5a88d986b1f85f0dc"
          }
        },
        "4a77b66da9224fd289ec33b445d2cf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5af422ad7f6d4e9385ca963b0fc81fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "324ea3ed7ea34d52a6f3a94ba50bfaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d70771e0cb0942b5a88d986b1f85f0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv6cJi70i65O"
      },
      "source": [
        "save_path = '/content/drive/MyDrive/EECS595-Fall2020/model.pt'\r\n",
        "train_data_path = '/content/drive/Shared drives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\r\n",
        "temperature = 0.1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628,
          "referenced_widgets": [
            "ad05459152aa46e38a66ee3c3376c21d",
            "b6d30ee506e545908a15e06663812552",
            "ba1cca0d307f47c7943de351697be09d",
            "db4efe0c16ef49c59ecb7d54e4936220",
            "f270904d9f4a432d8f51427b89c98b58",
            "ec415dab86e44d8aa082406cdca2d311",
            "327e487f673d4560ab7b323cd3a05c03",
            "359ac9f8823945688b0044264836ee68",
            "8b12b60ed0e14d7894f08bd62c78341f",
            "c23502029a0b4978818821d777c01bc1",
            "60bc6ebe503e445c877aed14d3989158",
            "237f12db3ba5460f83d5b9f3edb93082",
            "4a77b66da9224fd289ec33b445d2cf3e",
            "5af422ad7f6d4e9385ca963b0fc81fa5",
            "324ea3ed7ea34d52a6f3a94ba50bfaa0",
            "d70771e0cb0942b5a88d986b1f85f0dc"
          ]
        },
        "id": "II8XrEawiu7q",
        "outputId": "1da91812-cbec-426e-a189-009fd1850f3e"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "try:\r\n",
        "  import omegaconf\r\n",
        "except:\r\n",
        "  !pip install omegaconf\r\n",
        "  !pip install hydra-core\r\n",
        "  !pip install pytorch_lightning\r\n",
        "import json\r\n",
        "import random\r\n",
        "import tqdm\r\n",
        "from pytorch_lightning.metrics.functional.classification import auroc, accuracy, recall, precision\r\n",
        "from pytorch_lightning.metrics.functional import f1\r\n",
        "import math\r\n",
        "\r\n",
        "def kfold(tensor, index):\r\n",
        "  try:\r\n",
        "    return torch.cat([tensor[:index*tensor.shape[0]//10], tensor[(index+1)*tensor.shape[0]//10:]])\r\n",
        "  except:\r\n",
        "    return tensor[:index*len(tensor)//10] + tensor[(index+1)*len(tensor)//10:]\r\n",
        "\r\n",
        "def kfoldtest(tensor, index):\r\n",
        "  try:\r\n",
        "    return tensor[index*tensor.shape[0]//10:(index+1)*tensor.shape[0]//10]\r\n",
        "  except:\r\n",
        "    return tensor[index*len(tensor)//10:(index+1)*len(tensor)//10]\r\n",
        "\r\n",
        "def smooth_max(x, temp=1.0):\r\n",
        "  return torch.sum(torch.softmax(x/temp, dim=-1) * x, dim=-1)\r\n",
        "\r\n",
        "def pad_sentences(sents):\r\n",
        "  return sents + ['' for x in range(7-len(sents))]\r\n",
        "\r\n",
        "def get_pairs(words, label, all=False):\r\n",
        "  if all:\r\n",
        "    allpairs = [(i,j) for i in range(len(words)) for j in range(len(words)) if i < j and i!=j]\r\n",
        "  elif label == -1:\r\n",
        "    allpairs = random.sample([(i,j) for i in range(len(words)) for j in range(len(words)) if i < j and i!=j], 7)\r\n",
        "  else:\r\n",
        "    allpairs = [(i, label) for i in range(label)]\r\n",
        "  return [(words[i], words[j]) for i,j in allpairs]\r\n",
        "\r\n",
        "def compress_all_pairs(logits):\r\n",
        "  output = torch.stack([logits[i*(i+1)//2:(i+1)*(i+2)//2].max() for i in range(math.floor(math.sqrt(8*logits.shape[0]+1)-1)//2)])\r\n",
        "  return output\r\n",
        "\r\n",
        "def compress_all_pairs_smooth(logits, temp=1.0):\r\n",
        "  output = torch.stack([smooth_max(logits[i*(i+1)//2:(i+1)*(i+2)//2], temp) for i in range(math.floor(math.sqrt(8*logits.shape[0]+1)-1)//2)])\r\n",
        "  return output\r\n",
        "\r\n",
        "def score_accuracy(preds, labels, threshold):\r\n",
        "  correct = 0\r\n",
        "  for p, l in zip(preds, labels):\r\n",
        "    p2 = p > threshold\r\n",
        "    if l != -1:\r\n",
        "      if p2[l-1] and torch.all(~p2[:l-1]):\r\n",
        "        correct += 1\r\n",
        "    else:\r\n",
        "      if torch.all(~p2):\r\n",
        "        correct += 1\r\n",
        "  return correct/len(preds)\r\n",
        "\r\n",
        "def get_predictions(preds, threshold):\r\n",
        "  out = []\r\n",
        "  for p in preds:\r\n",
        "    p2 = p > threshold\r\n",
        "    if torch.any(p2):\r\n",
        "      out.append(torch.where(p2)[0][0].item()+1)\r\n",
        "    else:\r\n",
        "      out.append(-1)\r\n",
        "  return out\r\n",
        "\r\n",
        "def log1mexp(x):\r\n",
        "    return torch.where(x > -0.693, torch.log(-torch.expm1(x)), torch.log1p(-torch.exp(x)))\r\n",
        "\r\n",
        "\r\n",
        "# Download RoBERTa already finetuned for MNLI\r\n",
        "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\r\n",
        "roberta.register_classification_head('eat', num_classes=2)\r\n",
        "roberta.load_state_dict(torch.load('/content/drive/MyDrive/EECS595-Fall2020/model.pt'))\r\n",
        "#roberta.eval()  # disable dropout for evaluation\r\n",
        "roberta = roberta.cuda()\r\n",
        "\r\n",
        "from fairseq.data.data_utils import collate_tokens\r\n",
        "\r\n",
        "dev_file = '/content/drive/Shared drives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\r\n",
        "dev_data = json.load(open(dev_file))\r\n",
        "\r\n",
        "sentences = [x['story'] for x in dev_data]\r\n",
        "labels = torch.tensor([x['breakpoint'] for x in dev_data])\r\n",
        "tokenized_sents = sum([list(zip(pad_sentences(x[:-1]), pad_sentences(x[1:]))) for x in sentences], [])\r\n",
        "batch = collate_tokens(\r\n",
        "    [roberta.encode(pair[0], pair[1]) for pair in tokenized_sents[:7]], pad_idx=1\r\n",
        ")\r\n",
        "\r\n",
        "params = [x for n,x in roberta.named_parameters() if '.eat.' in n]\r\n",
        "# For frozen base use\r\n",
        "# opt = torch.optim.AdamW(params,lr=2e-5)\r\n",
        "opt = torch.optim.AdamW(roberta.parameters(),lr=2e-6)\r\n",
        "\r\n",
        "\r\n",
        "torch.set_printoptions(sci_mode=False)\r\n",
        "# Encode a pair of sentences and make a prediction\r\n",
        "split = 1\r\n",
        "\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "\r\n",
        "ema = 0\r\n",
        "denom = 0\r\n",
        "for epoch in range(50):\r\n",
        "  # Comment out this next line for frozen base\r\n",
        "  roberta.train()\r\n",
        "  for words, label in tqdm.tqdm_notebook(zip(kfold(sentences, split), kfold(labels, split))):\r\n",
        "    opt.zero_grad()\r\n",
        "    pairs = get_pairs(words, label, True)\r\n",
        "    batch = collate_tokens(\r\n",
        "      [roberta.encode(pair[0], pair[1]) for pair in pairs], pad_idx=1\r\n",
        "    )\r\n",
        "    logits = roberta.predict('eat', batch)[:,0]\r\n",
        "    predictions = compress_all_pairs_smooth(logits, temperature)\r\n",
        "    if label != -1:\r\n",
        "      loss = -log1mexp(predictions[:label])\r\n",
        "      loss[label-1] = -predictions[label-1]\r\n",
        "      loss = loss.mean()\r\n",
        "    else:\r\n",
        "      loss = -log1mexp(predictions).mean()\r\n",
        "    ema = 0.99*ema + loss.item()\r\n",
        "    denom = 0.99*denom + 1\r\n",
        "    loss.backward()\r\n",
        "    opt.step()\r\n",
        "\r\n",
        "  roberta.eval()\r\n",
        "  total = 0.0\r\n",
        "  count = 0\r\n",
        "  preds = []\r\n",
        "  bp_preds = []\r\n",
        "  ys = []\r\n",
        "  bp_ys = []\r\n",
        "  for words, label in zip(kfoldtest(sentences, split), kfoldtest(labels, split)):\r\n",
        "    pairs = get_pairs(words, label, True)\r\n",
        "    batch = collate_tokens(\r\n",
        "      [roberta.encode(pair[0], pair[1]) for pair in pairs], pad_idx=1\r\n",
        "    )\r\n",
        "    with torch.no_grad():\r\n",
        "      logits = roberta.predict('eat', batch)[:,0]\r\n",
        "      logitsc = compress_all_pairs_smooth(logits, temperature)\r\n",
        "      bp_preds.append(torch.exp(logitsc).cpu())\r\n",
        "      bp_ys.append(label)\r\n",
        "      prediction = logits.max()\r\n",
        "      preds.append(torch.exp(prediction).cpu())\r\n",
        "      ys.append((label != -1).float().cpu())\r\n",
        "      if label != -1:\r\n",
        "        loss = -log1mexp(logitsc[:label])\r\n",
        "        loss[label-1] = -logitsc[label-1]\r\n",
        "        loss = loss.mean()\r\n",
        "      else:\r\n",
        "        loss = -log1mexp(logitsc).mean()\r\n",
        "        total += loss.item()\r\n",
        "        count += 1\r\n",
        "  torch.save(roberta.state_dict(), save_path)\r\n",
        "  preds = torch.stack(preds, dim=0)\r\n",
        "  ys = torch.stack(ys, dim=0)\r\n",
        "  accs = torch.tensor([accuracy(preds > i/100.0, ys) for i in range(100)])\r\n",
        "  score_accs = torch.tensor([score_accuracy(bp_preds, bp_ys, i/100.0) for i in range(100)])\r\n",
        "  final_preds = get_predictions(bp_preds, score_accs.argmax().item()/100.0)\r\n",
        "  final_preds = [4 if x!=-1 else -1 for x in final_preds]\r\n",
        "  f1_score = f1(torch.tensor([x if x!=-1 else 0 for x in final_preds]), torch.tensor([x if x!=-1 else 0 for x in bp_ys]), 6)\r\n",
        "  recall_score = recall(torch.tensor([x if x!=-1 else 0 for x in final_preds]), torch.tensor([x if x!=-1 else 0 for x in bp_ys]), 6, 'macro')\r\n",
        "  precision_score = precision(torch.tensor([x if x!=-1 else 0 for x in final_preds]), torch.tensor([x if x!=-1 else 0 for x in bp_ys]), 6, 'macro')\r\n",
        "  print(f'f1: {f1_score.item()}, recall:{recall_score.item()}, precision:{precision_score.item()}, accuracy:{accuracy(torch.tensor([1 if x!=-1 else 0 for x in final_preds]), torch.tensor([1 if x!=-1 else 0 for x in bp_ys]), 6).item()}')\r\n",
        "  print(f'Threshold 1: {accs.argmax().item()/100.0}, Threshold 2: {score_accs.argmax().item()/100.0}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:113: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad05459152aa46e38a66ee3c3376c21d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "f1: 0.6538461446762085, recall:0.26787880063056946, precision:0.2142857164144516, accuracy:0.8269230723381042\n",
            "Threshold 1: 0.09, Threshold 2: 0.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: RuntimeWarning: You have set 6 number of classes which is different from predicted (2) and target (2) number of classes\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b12b60ed0e14d7894f08bd62c78341f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-db2d35be12aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mema\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdenom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2dB21bit3Nq"
      },
      "source": [
        "test_path = '/content/drive/Shared drives/EECS595-Fall2020/Final_Project_Common/EAT/eat_test_unlabeled.json'\r\n",
        "threshold = 0.35"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSZPUu_xjGuJ",
        "outputId": "8f938610-f659-4997-a75f-6bcc4ba7493e"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "try:\r\n",
        "  import omegaconf\r\n",
        "except:\r\n",
        "  !pip install omegaconf\r\n",
        "  !pip install hydra-core\r\n",
        "  !pip install pytorch_lightning\r\n",
        "import json\r\n",
        "import random\r\n",
        "import tqdm\r\n",
        "from pytorch_lightning.metrics.functional.classification import auroc, accuracy, recall, precision\r\n",
        "from pytorch_lightning.metrics.functional import f1\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "def pad_sentences(sents):\r\n",
        "  return sents + ['' for x in range(7-len(sents))]\r\n",
        "\r\n",
        "def get_pairs(words, label, all=False):\r\n",
        "  if all:\r\n",
        "    allpairs = [(i,j) for i in range(len(words)) for j in range(len(words)) if i < j and i!=j]\r\n",
        "  elif label == -1:\r\n",
        "    allpairs = random.sample([(i,j) for i in range(len(words)) for j in range(len(words)) if i < j and i!=j], 7)\r\n",
        "  else:\r\n",
        "    allpairs = [(i, label) for i in range(label)]\r\n",
        "  return [(words[i], words[j]) for i,j in allpairs]\r\n",
        "\r\n",
        "def compress_all_pairs(logits):\r\n",
        "  output = torch.stack([logits[i*(i+1)//2:(i+1)*(i+2)//2].max() for i in range(math.floor(math.sqrt(8*logits.shape[0]+1)-1)//2)])\r\n",
        "  return output\r\n",
        "\r\n",
        "def get_predictions(preds, threshold):\r\n",
        "  out = []\r\n",
        "  for p in preds:\r\n",
        "    p2 = p > threshold\r\n",
        "    if torch.any(p2):\r\n",
        "      out.append(torch.where(p2)[0][0].item()+1)\r\n",
        "    else:\r\n",
        "      out.append(-1)\r\n",
        "  return out\r\n",
        "\r\n",
        "\r\n",
        "# Download RoBERTa already finetuned for MNLI\r\n",
        "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\r\n",
        "roberta.register_classification_head('eat', num_classes=2)\r\n",
        "roberta.load_state_dict(torch.load(save_path))\r\n",
        "roberta.eval()  # disable dropout for evaluation\r\n",
        "roberta = roberta.cuda()\r\n",
        "\r\n",
        "from fairseq.data.data_utils import collate_tokens\r\n",
        "\r\n",
        "# Load Data\r\n",
        "dev_file = test_path\r\n",
        "dev_data = json.load(open(dev_file))\r\n",
        "\r\n",
        "sentences = [x['story'] for x in dev_data]\r\n",
        "ids = [x['id'] for x in dev_data]\r\n",
        "tokenized_sents = sum([list(zip(pad_sentences(x[:-1]), pad_sentences(x[1:]))) for x in sentences], [])\r\n",
        "batch = collate_tokens(\r\n",
        "    [roberta.encode(pair[0], pair[1]) for pair in tokenized_sents[:7]], pad_idx=1\r\n",
        ")\r\n",
        "\r\n",
        "#Make Predictions\r\n",
        "bp_preds = []\r\n",
        "for words in sentences:\r\n",
        "  pairs = get_pairs(words, None, True)\r\n",
        "  batch = collate_tokens(\r\n",
        "    [roberta.encode(pair[0], pair[1]) for pair in pairs], pad_idx=1\r\n",
        "  )\r\n",
        "  with torch.no_grad():\r\n",
        "    logits = roberta.predict('eat', batch)[:,0]\r\n",
        "    logitsc = compress_all_pairs(logits)\r\n",
        "    bp_preds.append(torch.exp(logitsc).cpu())\r\n",
        "\r\n",
        "\r\n",
        "final_preds = get_predictions(bp_preds, threshold)\r\n",
        "outputs = [{'id': id, 'pred_label': 1 if pred == -1 else 0, 'pred_breakpoint': pred} for id, pred in zip(ids, final_preds)]\r\n",
        "print(outputs)\r\n",
        "with open('predictions.json', 'w') as f:\r\n",
        "  f.write(json.dumps(outputs))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 32.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 37.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 21.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 23.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 21.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 18.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44620 sha256=7cee7669d94ced96fb741413967d35a17caa36cfb73d8159bbf909b654de717c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML, omegaconf\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 omegaconf-2.0.5\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/1f/7f502b9e37596164111655861370b08626f46f9e4524433c354f472765d4/hydra_core-1.0.4-py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core) (3.3.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: omegaconf>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from hydra-core) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.5->hydra-core) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.5->hydra-core) (5.3.1)\n",
            "Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.5->hydra-core) (0.8)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141231 sha256=2a4bc9ee6a2998668018dc85923dd966d39712254adcc5034ff8ad1eef75afa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.8 hydra-core-1.0.4\n",
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/7b/0d5ef515a695fa55a29786297f9e8bd6e0f35a689decd53574cdd80597bc/pytorch_lightning-1.1.1-py3-none-any.whl (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 15.6MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.3.0)\n",
            "Collecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.34.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=f1270a2e644b0d655426c0edacb95dc7f268734fc8b10378c13c97e7fb54c9c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, fsspec, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed fsspec-0.8.5 future-0.18.2 pytorch-lightning-1.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/fairseq\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/fairseq\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/data\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/data\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so -> fairseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 751652118/751652118 [00:26<00:00, 28364866.31B/s]\n",
            "1042301B [00:00, 2490997.07B/s]\n",
            "456318B [00:00, 1509489.76B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'id': 'test_0', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_1', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_2', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_3', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_4', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_5', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_6', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_7', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_8', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_9', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_10', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_11', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_12', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_13', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_14', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_15', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_16', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_17', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_18', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_19', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_20', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_21', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_22', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_23', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_24', 'pred_label': 0, 'pred_breakpoint': 5}, {'id': 'test_25', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_26', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_27', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_28', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_29', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_30', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_31', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_32', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_33', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_34', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_35', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_36', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_37', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_38', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_39', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_40', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_41', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_42', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_43', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_44', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_45', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_46', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_47', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_48', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_49', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_50', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_51', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_52', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_53', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_54', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_55', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_56', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_57', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_58', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_59', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_60', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_61', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_62', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_63', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_64', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_65', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_66', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_67', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_68', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_69', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_70', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_71', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_72', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_73', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_74', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_75', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_76', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_77', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_78', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_79', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_80', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_81', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_82', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_83', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_84', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_85', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_86', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_87', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_88', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_89', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_90', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_91', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_92', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_93', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_94', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_95', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_96', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_97', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_98', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_99', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_100', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_101', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_102', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_103', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_104', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_105', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_106', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_107', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_108', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_109', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_110', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_111', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_112', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_113', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_114', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_115', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_116', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_117', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_118', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_119', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_120', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_121', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_122', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_123', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_124', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_125', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_126', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_127', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_128', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_129', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_130', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_131', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_132', 'pred_label': 0, 'pred_breakpoint': 5}, {'id': 'test_133', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_134', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_135', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_136', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_137', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_138', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_139', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_140', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_141', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_142', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_143', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_144', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_145', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_146', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_147', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_148', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_149', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_150', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_151', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_152', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_153', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_154', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_155', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_156', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_157', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_158', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_159', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_160', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_161', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_162', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_163', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_164', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_165', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_166', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_167', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_168', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_169', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_170', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_171', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_172', 'pred_label': 0, 'pred_breakpoint': 5}, {'id': 'test_173', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_174', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_175', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_176', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_177', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_178', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_179', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_180', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_181', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_182', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_183', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_184', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_185', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_186', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_187', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_188', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_189', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_190', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_191', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_192', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_193', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_194', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_195', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_196', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_197', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_198', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_199', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_200', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_201', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_202', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_203', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_204', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_205', 'pred_label': 0, 'pred_breakpoint': 5}, {'id': 'test_206', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_207', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_208', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_209', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_210', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_211', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_212', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_213', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_214', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_215', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_216', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_217', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_218', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_219', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_220', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_221', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_222', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_223', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_224', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_225', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_226', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_227', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_228', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_229', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_230', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_231', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_232', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_233', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_234', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_235', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_236', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_237', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_238', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_239', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_240', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_241', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_242', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_243', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_244', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_245', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_246', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_247', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_248', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_249', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_250', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_251', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_252', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_253', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_254', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_255', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_256', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_257', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_258', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_259', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_260', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_261', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_262', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_263', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_264', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_265', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_266', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_267', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_268', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_269', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_270', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_271', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_272', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_273', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_274', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_275', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_276', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_277', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_278', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_279', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_280', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_281', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_282', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_283', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_284', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_285', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_286', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_287', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_288', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_289', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_290', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_291', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_292', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_293', 'pred_label': 0, 'pred_breakpoint': 1}, {'id': 'test_294', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_295', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_296', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_297', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_298', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_299', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_300', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_301', 'pred_label': 1, 'pred_breakpoint': -1}, {'id': 'test_302', 'pred_label': 0, 'pred_breakpoint': 3}, {'id': 'test_303', 'pred_label': 0, 'pred_breakpoint': 4}, {'id': 'test_304', 'pred_label': 0, 'pred_breakpoint': 2}, {'id': 'test_305', 'pred_label': 1, 'pred_breakpoint': -1}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}