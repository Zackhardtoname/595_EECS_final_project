{
    # notes
    # lowercase bool names for hjson

    # debugging
    "to_trim": true

    # hparams
    "num_samples": 6
    "max_epochs": 5.0

    # bert base best
    # "lr": tune.choice([2e-5]),
    # "batch_size": tune.choice([32]),
    # "max_seq_length": tune.choice([32])
    # DEFAULT_4873e_00000_0_hidden_act=gelu,hidden_dropout_prob=0.1,lr=7.4593e-05,max_seq_length=48_2020-12-13_06-42-30/tb_logs/csqa/version_0

    # bert large
    # "lr": tune.loguniform(1e-6, 1e-4),
    # "batch_size": tune.choice([48, 32, 8]),
    # "max_seq_length": tune.choice([80, 48]),
    # "hidden_dropout_prob": tune.choice([.1, .2]),
    # "hidden_act": tune.choice(["gelu"]),
    # "architecture": {
    #     "tokenizer": BertTokenizer,
    #     "model": BertForMultipleChoice,
    #     "pretrained_model_name": "bert-large-uncased"
    # }


    # albert
    # "lr": tune.loguniform(1e-6, 1e-1),
    # "batch_size": tune.choice([48, 32]),
    # "max_seq_length": tune.choice([48, 32, 16]),
    # "hidden_dropout_prob": tune.choice([.1, .3, .5]),
    # "hidden_act": tune.choice(["relu", "gelu"]),
    # "architecture": {
    #     "tokenizer": AlbertTokenizer,
    #     "model": AlbertForMultipleChoice,
    #     "pretrained_model_name": "albert-base-v2"
    # }

    # unused
    "warmup_proportion": 0.1
    "use_tpu": false
    "tpu_name": None

    # probably fixed
    "do_lower_case": true
    "save_checkpoints_steps": 1000
    "iterations_per_loop": 1000
    "split": rand
    "test_size": 1200
    "seed": 42
    "use_gpu": 1

    "bert_config_file": bert
    "vocab_file": bert/vocab.txt
    "init_checkpoint": ./bert_model.ckpt
    "data_dir": data
    "output_dir": ./output
    "ckpt_dir": 'ckpt'
}